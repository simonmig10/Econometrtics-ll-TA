---
title: "lecture 3 presentation"
author:
- \emph{Simon Fløj Thomsen}^[Aalborg University, sft@business.aau.dk, lokale 24 fib 11, MaMTEP] 
date: \emph{`r format(Sys.time(), '%B %d, %Y')`}
output:
  beamer_presentation: 
    toc: true
    slide_level: 2 
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    
---





# Statistik opsumering


## Expected value

$$E(aX + b) = aE(X) + b$$
$$E(X+Y)= E(X)E(Y)$$

### 
$$E(XY)= E(X)E(Y)$$

- Hvis X og Y er uafhængige 

- Bevis for dette behøves i ik at kende 

## Variance 

$$Var(X) = E[X-E(X)]^2 = E(XX) - E(X)E(X)$$

- Vi kan se hvis $E(X)=0$ er $Var(X) = E(X^2)$

- Bevis ovenstående

$$Var(aX+b) = a^2 Var(X)$$
$$Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)$$

- Bevis ovenstående
$$Var(X + Y) = Var(X) + Var(Y)$$

- Hvis X og Y er uafhængige og dermed $Cov(X,Y)=0$




## Bevis for Var(X)

$$Var(X) = E[X-E(X)]^2 = E(XX) - E(X)E(X)$$


- Løs på tavlen 


## Bevis for Var(X+Y)


$$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$$


## Covariance 

$$Cov(X,Y) = E[X-E(X)][Y-E(Y)]= E(XY) - E(X)E(Y)$$

- Bevis ovenstående

$$Cov(X,X) = Var(X)$$

$$Cov(aX + b, cY + d) = acCov(X,Y)$$

$$Cov(X + Y, Z) = Cov(X,Z) + Cov(Y,Z)$$



## Bevis Cov(X,Y)

- Forsøg at bevis nedenstående: 

$$Cov(X,Y) = E[X-E(X)][Y-E(Y)]= E(XY) - E(X)E(Y)$$

- Løs på tavlen


# Moving Average process opsumering: 

## Mean, varians og Covariance


$$Y_t = \mu + \varepsilon_t + \alpha \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$

- Calculate mean of $Y_t$

$$E[Y_t] = E[\mu]  E[\varepsilon_t] + E[\alpha \varepsilon_{t-1}]$$

- Vi ved $E[\varepsilon_t] = E[\varepsilon_{t-1}] = 0$

$$E[Y_t] = \mu$$


## Mean, varians og Covariance

$$Y_t = \mu + \varepsilon_t + \alpha \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$


- Calculate the variance 
$$Var[Y_t] = E[(Y_t - \mu)^2]$$
$$=E[(\varepsilon_t + \alpha \varepsilon_{t-1})^2]$$
$$=E[\varepsilon_t^2  + 2\alpha \varepsilon_t \varepsilon_{t-1} + \alpha^2\varepsilon_{t-1}^2]$$
$$=E[\varepsilon_t^2]  + 2\alpha E[\varepsilon_t \varepsilon_{t-1}] +  \alpha^2 E[\varepsilon_{t-1}^2]$$

- Hvorfor ved vi $2\alpha E[\varepsilon_t \varepsilon_{t-1}] = 0$?
- Hvad sker der med $E[\varepsilon_t^2]$ og $E[\varepsilon_{t-1}^2]$, og hvorfor?

$$=E[\varepsilon_t^2] + \alpha^2 E[\varepsilon^2_{t-1}]$$
$$(1 + \alpha^2) \sigma^2$$


## Mean, varians og Covariance

$$Y_t = \mu + \varepsilon_t + \alpha \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$

- Calculate autocovariance between $Y_t$ and $Y_{t-1}$

$$Cov[Y_t,Y_{t-1}] = E[(Y_t-\mu)*(Y_{t-1} - \mu)]$$
$$E[(\varepsilon_t + \alpha \varepsilon_{t-1}) (\varepsilon_{t-1} + \alpha \varepsilon_{t-2})]$$

- Forsøg selv at tage de sidste steps!

$$\alpha E[\varepsilon_{t-1}]$$

$$\alpha \sigma^2$$

## Mean, Variance og Covariance

- Calculate autocovariance between $Y_t$ and $Y_{t-2}$

$$Cov[Y_t,Y_{t-2}] = E[(Y_t-\mu)*(Y_{t-2} - \mu)]$$
$$= E[(\varepsilon_t + \alpha \varepsilon_{t-1}) (\varepsilon_{t-2} + \alpha \varepsilon_{t-3})]$$

- Samme metode som vist på tavlen før:

$$=0$$


## MA(1) Simulation 

$$Y_t = 2 + \varepsilon_t + 0.9 \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, 1.5^2)$$

```{r, echo=F, fig.width=7, fig.height=4}
set.seed(1393)

n=1000 # set time period to 500 obs
e <- rnorm(n, mean=0, sd=1.5) # purely random process with mean 0 and sd = 1.5
mu=2 # we set mu= 2
y <- c()
for (t in 2:n) {
y[t] <- mu + e[t] + 0.9*e[t-1] # Note: we set alpha = 0.9$
y <- y[!is.na(y)]
}
ts.plot(y, main=expression(paste("MA(1) process with ", alpha, "=0.9")), xlab="")
```

## MA(1) Simulation 

```{r, eval=FALSE}
mean(y)# 1.919655
var(y) # 3.991872
cov(y[-length(y)],y[-1])# 1.922769
```
$$E(y_t)= \mu= 2$$
$$V(y_t)= (1+\alpha^2)\sigma^2$$
$$V(y_t)= (1+ 0.9^2)1.5^2$$
$$V(y_t)= (1+ 0.9^2)1.5^2 = 4.0725$$
$$Cov= \alpha* \sigma^2$$
$$Cov= 0.9* 1.5^2= 2.025$$



# Auto regressive Processes opsumering


$$Y_t = \mu  + \theta Y_{t-1}+ \varepsilon_t$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$

- Værktøjer vi skal bruge til properties af AR-modeller: 

1. Geometriske serier. 

2. Difference ligninger. 


## Geometriske serier review - Eksempler på serier

### Eksempel 1:

$$\sum^n_{n=1} x^n=x + x^2 + x^3 + x^4 +...+x^{n-1}$$


### Eksempel 2:

$$x + \sqrt{x} + 1 + \frac{1}{\sqrt{x}} + \frac{1}{x} ... $$

## Typer af geometriske serier

Note til senere: 

- $a= \mu$
- $k = \theta$


### Endelig serie

$\sum^n_{n=1} ak^n = a* \frac{1-k^n}{1-k}$,   $k \neq1$

### Uendelig serie

$\sum^n_{n=1} ak^n = \frac{a}{1-k}$,   $\lvert k \rvert  <1$


$\sum^n_{n=1} ak^n = na$,   $\lvert k \rvert  =1$



## Udregning

### Endelig serie

$$S_n = \alpha + \alpha*k + \alpha*k^2 + \alpha*k^3 + ... + \alpha*k^{n-1}$$
$$k*S_n = \alpha*k + \alpha*k^2 + \alpha*k^3 + \alpha*k^4 + ... + \alpha*k^{n}$$
$$S_n - k*S_n = \alpha + (\alpha*k - \alpha*k) + (\alpha*k^2- \alpha*k^2) + ... + (\alpha*k^{n-1} - \alpha*k^{n-1}) - \alpha*k^{n}$$
$$S_n - k*S_n = \alpha - \alpha*k^{n}$$
$$S_n(1-k) = \alpha(1-k^n)$$

$$S_n = \alpha * \frac{1-k^n}{1-k}$$

## Udregning

### Uendelig serie

Hvis $\vert k \vert <  1$ når $n \to \infty$ vil udtrykket gå mod:

$$S_n = \alpha * \frac{1}{1-k}$$

Dermed kan vi skrive:

$$ \sum_{n=1}^\infty \alpha k^n = \frac{ \alpha}{1-k}$$

## Difference equations: Math to econometrics

Looking at a 1. order difference equations

In 2. semester math seen like this: 

$$x_t = ax_{t-1} + b$$
In time series econometrics you have seen it like this (AR(1) process): 

$$y_t= \mu + \theta y_{t-1} + \varepsilon_t$$

What are the differences?

Lets first see what they got in common:

- Both equations got a constant $\mu$ and $b$

- Both equations got a coefficient $\theta$ and $a$

- Both equations got a variable that changes over time (discrete) $y_t$ and $x_t$


## Difference equations: Math to econometrics

**The difference is the error term $\varepsilon_t$ with the diffenition:**

$$\varepsilon_t \sim IID(0,\sigma^2)$$
Identical, Independent Distributed med $mean = 0$ og $Var= \sigma^2$

Later we take a look at how this changes things!


## Differens equations (Math)

Lets solve the difference equation we saw before:

$$x_t = ax_{t-1} + b_t$$
We can start from a given point $x_0$ 

$$x_1 = ax_0 + b_1$$
$$x_2 = ax_1 + b_2 = a(ax_0 + b_1) + b_2 = a^2 x_0 + ab_1 + b_2$$
$$x_3 = ax_2 + b_3 = a(a^2x_0 + ab_1 + b_2) + b_3 = a^3x_0 + a^2 b_1 + ab_2 + b_3$$

## Differens equations (Math)

We can already see the pattern:

$$x_t= a^t x_0 + \sum^t_{k=1} a^{t-k} b_k$$

We now assume the case when $b_k = b$ so we now got a constant as in the AR(1) model (fixed over time).

So we can now write the last term as:

$$\sum^t_{k=1}a^{t-k}b$$ 
Which is a geometric series! that we just covered!
$$\sum^t_{k=1}a^{t-k}b= b(a^{t-1} + a^{t-2} + a^{t-3} +...+a +1) =\frac{(b-ba^t)}{(1-a)}$$

## Differens equations (Math)

Therefor we can now write: 

$$x_t= a^t (x_0 - \frac{b}{1-a}) + \frac{b}{1-a}$$

We can see that if $x_0 = \frac{b}{1-a}$ we get that $x_t= \frac{b}{1-a}$ which I have illustrated down below:

```{r, echo=FALSE}
knitr::include_graphics(rep("diff..png", 1), dpi = 300)
```


## Differens equations (Math)

In fact if just $x_s$ at any point hits $\frac{b}{1-a}$ we wont get away from it as:

$$x_{s+1} = a\frac{b}{1-a} +b = \frac{b}{1-a}$$

```{r, echo=FALSE}
knitr::include_graphics(rep("diff2.png", 1), dpi = 300)
```

But what if we never hit that value?

## Difference equations (stability)

### Case 1

$$\vert a \vert < 1$$
We then see that $a^t$ goes towards 0 as $t \to \infty$ in: 

$$x_t= a^t (x_0 - \frac{b}{1-a}) + \frac{b}{1-a}$$
And we will end up with 

$$x_t= \frac{b}{1-a}$$

## Difference equations (stability)

### Case 2

$$\vert a \vert > 1$$

- We then see that $a^t$ goes towards $\infty$ as $t \to \infty$ and will explode.


- Lets look at the different scenarios


## Difference equations (stability)

```{r, echo=FALSE}
knitr::include_graphics(rep("diff3.png", 1), dpi = 150)
```

## AR(1) model Econometrics

Lets look at the AR(1) process again:

$$y_t= \mu + \theta y_{t-1} + \varepsilon_t$$
Where the only difference was the error term: $\varepsilon_t$ lets see some examples and what the difference is


## AR(1) model Exonometrics

To give an example we use the model:
$$y_t= 5 +  0.5y_{t-1}+\varepsilon_t$$
With start value $y_0=0$

Lad is bruge løsningen for en difference ligning for når $\lvert a \rvert < 1$

```{r, echo=FALSE, fig.height= 3.3}
suppressPackageStartupMessages(library(forecast))
# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n)

# loop to create x
for (t in 2:n) x[t] <- 5+ 0.5 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 0.5*X_t-1 + u_t"); abline( h=10, col= "red")
```
We see the shocks from $\varepsilon_t$ does so we never stay in $\frac{b}{1-a}$, så istedet regner vi mean!

## AR(1) model Exonometrics

$$y_t= 5 +  1.5y_{t-1}+\varepsilon_t$$


```{r, echo=FALSE, fig.height= 5}

# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n)

# loop to create x
for (t in 2:n) x[t] <- 5+ 1.5 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 1.5*X_t-1 + u_t")

```


## AR(1) model Exonometrics

$$y_t= 5 +  1y_{t-1}+\varepsilon_t$$

```{r, echo=FALSE, fig.height= 5}
# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n, mean = 0, sd= 1)

# loop to create x
for (t in 2:n) x[t] <- 5+ 1 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 1*X_t-1 + u_t")
```

## AR(1) model Exonometrics

Plottet vi så før ligner bare en lineær funktion! Hvilket det er!

Husk løsning til vores difference eq.

$$x_t = a^t x_0  + \sum^t_{k=1} a^{t-k} b_k$$
Her brugte vi løsningen til den geometriske serie til at substituere ind for $\sum^t_{k=1} a^{t-k} b_k$ Men vi antog $\lvert a \rvert < 1$

Gå tilbage til geometriske serier!

## AR(1) model Exonometrics

Vi kan derfor nu indsætte $\sum^t_{k=1} a^{t-k} b_k = ta$

$$x_t = a^t x_0  + ta$$
Vi ved $a=1$ dermed starter vi i $x_0$ og vokser linært som $t \rightarrow \infty$

## AR(1) model Exonometrics

Tilbage til AR(1) processen! Hvad er det vi kalder det når $\lvert a \rvert = 1$ aka $\lvert \theta \lvert = 1$ 

En Random walk med drift!

Lad os lave det lidt mere tydeligt ved at øge standard deviation i fejlledet!


```{r, echo=FALSE, fig.height= 5}
# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n, mean = 0, sd= 50)

# loop to create x
for (t in 2:n) x[t] <- 5+ 1 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 1*X_t-1 + u_t")
```


## AR(1) model Exonometrics (mean)

Som vi såå før grundet fejlledet er der ik en løsning $\frac{b}{1-a}$ men istedet kan vi udregne mean

$$E[y_t] = E[\mu + \theta y_{t-1} + \varepsilon_t]$$
$$= \mu + \theta E[y_{t-1}] + E[\varepsilon]$$
$$= \mu + \theta E[\mu + \theta y_{t-2} + \varepsilon_{t-1}]$$
$$= \mu + \mu\theta+  \theta^2 E[y_{t-2}]$$
$$= \mu + \mu\theta+  \theta^2 E[\mu + \theta y_{t-3} + \varepsilon_{t-2}]$$
$$= \mu(1 + \theta + \theta^2 + \theta^3 + ... +\theta^\infty)$$


So back to the geometric series if $\vert \theta \vert< 1$ we get $\frac{\mu}{1-\theta}$


## AR(1) model Exonometrics (mean)

Lets try calculating the mean using the example from before 

$$y_t= 5 +  0.5y_{t-1}+\varepsilon_t$$
$$E[y_t]= \frac{5}{1-0.5} = 10$$

Lets look at the plot again!


## AR(1) model Exonometrics (Variance)

Da $\mu$ er en konstant vil denne ikke påvirke variancen og vi kan fjerne denne fra start. 

```{r, echo=FALSE, fig.height= 3.5}
set.seed(213)
n=500
e <- rnorm(n, mean = 0, sd = 1.5)
Y <- 2/(1-0.9)
for (t in 2:n) {
Y[t] <- 2+ 0.9*Y[t-1]+e[t]
}
ts.plot(Y, main=expression(paste("AR(1) process with ", theta, "=0.9 and" , mu ,"=2")), xlab="")
```
```{r}
var(Y)
```

## AR(1) model Exonometrics (Variance)

```{r,  echo=FALSE, fig.height= 3.5}
set.seed(213)
n=500
e <- rnorm(n, mean = 0, sd = 1.5)
Y <- 6/(1-0.9)
for (t in 2:n) {
Y[t] <- 6+ 0.9*Y[t-1]+e[t]
}
ts.plot(Y, main=expression(paste("AR(1) process with ", theta, "=0.9 and" , mu ,"=6")), xlab="")
```

```{r}
var(Y)
```


## AR(1) model Exonometrics (Variance)

$$y_t= \mu +  \theta y_{t-1}+\varepsilon_t$$
$$\varepsilon_t \sim IID(0,\sigma^2)$$
Vi antager derfor $\mu = 0$

$$V(y_t) = E[(y_t - E[y_t])^2]$$

- forklar dette step
$$V(y_t) = E[(\theta y_{t-1} + \varepsilon_t)^2]$$
$$V(y_t) = E[\varepsilon_t^2] + \theta^2 E[y_{t-1}^2]$$
$$V(y_t) = \sigma^2 + \theta^2 E[y_{t-1}^2]$$
$$V(y_t) = \sigma^2 + \theta^2 E[(\theta y_{t-2} + \varepsilon_{t-1})^2]$$

- Vi kan nu indsætte $y_{t-2}$ og gøre nøjagtigt de samme steps:
$$\sigma^2 ( 1+ \theta^2 + \theta^4 + \theta^6 + ... + \theta^{\infty})$$

## AR(1) model Exonometrics (Variance)



- Brug igen geometrisk serier
$$\frac{\sigma^2} {1 - \theta^2}$$
- IF $\lvert \theta \rvert < 1$

## AR(1) model Exonometrics (Auto covariance)


$$cov(Y_t , Y_{t-1}) = cov(\mu +  \theta Y_{t-1}+\varepsilon_t, Y_{t-1})$$

- fra statistik ved vi at $Cov(X + Y, Z) = Cov(X,Z) + Cov(Y,Z)$

$$cov(\mu,Y_{t-1}) +\theta cov(Y_{t-1},Y_{t-1}) + cov(\varepsilon_t,Y_{t-1})$$

- vi ved at $cov(\mu,Y_{t-1}) = cov(\varepsilon_t,Y_{t-1}) = 0$

- Og hvad er det nu $cov(Y_{t-1},Y_{t-1})$ er? 

$$cov(Y_t , Y_{t-1}) = \theta \frac{\sigma^2}{1-\theta^2}$$ 
- Og antagelsen fra variance skal nu bruges: $\lvert \theta \rvert < 1$




## AR(1) model Exonometrics (Auto covariance)

$$cov(Y_t , Y_{t-2}) = cov(\mu +  \theta Y_{t-1}+\varepsilon_t, Y_{t-2})$$

$$cov(\mu,Y_{t-2}) +\theta cov(Y_{t-1},Y_{t-2}) + cov(\varepsilon_t,Y_{t-1})$$

- vi ved at $cov(\mu,Y_{t-1}) = cov(\varepsilon_t,Y_{t-1}) = 0$

- Og vi kender $cov(Y_{t-1},Y_{t-2})$ som vi fandt på sidste slide. 

$$cov(Y_t , Y_{t-2}) = \theta^2 \frac{\sigma^2}{1-\theta^2}$$ 

- Da vi bruger covariancen med antagelsen, gælder den stadig: $\lvert \theta \rvert < 1$

- Derfor ACF aftager over tid når i plotter en AR-model.


# Done

## Cheatsheet

+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Name        | AR(1)                             | MA(1)                    | AR(1) (RW1)             | AR(1) (RW2)             | AR(1) (RW3)             |
|             |                                   |                          |                         |                         |                         |
|             | $$                                |                          | $$                      | $$                      | $$                      |
|             | \lvert\theta \lvert < 1           |                          | \lvert\theta \lvert = 1 | \lvert\theta \lvert = 1 | \lvert\theta \lvert = 1 |
|             | $$                                |                          | $$                      | $$                      | $$                      |
+=============+===================================+==========================+=========================+=========================+=========================+
| Mean        | $$                                | $$                       | $$                      | $$                      | $$                      |
|             | \frac{\mu}{1-\theta}              | \mu                      | Y_0                     | Y_0 + T\mu              | Y_0 + T\mu + t          |
|             | $$                                | $$                       | $$                      | $$                      | $$                      |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Var         | $$                                | $$                       | $$                      | $$                      | $$                      |
|             | \frac{\sigma^2}{1-\theta^2}       | (1 + \alpha ^2) \sigma^2 | T \sigma^2              | T \sigma^2              | T \sigma^2              |
|             | $$                                | $$                       | $$                      | $$                      | $$                      |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Cov         | $$                                | $$                       | $$                      | $$                      | $$                      |
|             | \theta\frac{\sigma^2}{1-\theta^2} | \alpha \sigma^2          | T \sigma^2              | T \sigma^2              | T \sigma^2              |
|             | $$                                | $$                       | $$                      | $$                      | $$                      |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Stationary? |           YES!                    | YES!                     | NO!                     | NO!                     | NO!                     |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+





