---
title: "lecture 3 presentation"
author:
- \emph{Simon Fløj Thomsen}^[Aalborg University, sft@business.aau.dk, lokale 24 fib 11, MaMTEP] 
date: \emph{`r format(Sys.time(), '%B %d, %Y')`}
output:
  beamer_presentation: 
    toc: true
    slide_level: 2 
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
    
---

# Kort intro:

- Slides ligger nederst på moodle

- Alle skulle gerne kunne udregne varians, gennemsnit og autocovarians for AR(1) og MA(1) modeller. 


- Alle skulle gerne kunne forklarer hvilke regler der bliver brugt til at udregne ovenstående. 


- Hvis tid til overs kan i lave eksamensopgaver. 

# Statistik opsumering


## Expected value
###
$$E(aX + b) = aE(X) + b$$

###
$$E(X+Y)= E(X)+E(Y)$$

### 
$$E(XY)= E(X)E(Y)$$

- Hvis X og Y er uafhængige 

- Bevis for dette behøves i ik at kende 

## Variance 
###
$$Var(X) = E[X-E(X)]^2 = E(XX) - E(X)E(X)$$

- Vi kan se hvis $E(X)=0$ er $Var(X) = E(X^2)$

- Bevis ovenstående

###
$$Var(aX+b) = a^2 Var(X)$$

###
$$Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)$$

- Bevis ovenstående
$$Var(X + Y) = Var(X) + Var(Y)$$

- Hvis X og Y er uafhængige og dermed $Cov(X,Y)=0$




## Bevis for Var(X)

$$Var(X) = E[X-E(X)]^2 = E(XX) - E(X)E(X)$$


- Løs på tavlen 


## Bevis for Var(X+Y)


$$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$$


## Covariance 
###
$$Cov(X,Y) = E[X-E(X)][Y-E(Y)]= E(XY) - E(X)E(Y)$$

- Bevis ovenstående

###
$$Cov(X,X) = Var(X)$$

###
$$Cov(aX + b, cY + d) = acCov(X,Y)$$

- Konstanter ændrer Covariansen men ikke correlationen!

###
$$Cov(X + Y, Z) = Cov(X,Z) + Cov(Y,Z)$$



## Bevis Cov(X,Y)

- Forsøg selv at bevis nedenstående (5-10 minutter): 

$$Cov(X,Y) = E[X-E(X)][Y-E(Y)]= E(XY) - E(X)E(Y)$$

- Løs på tavlen


# Moving Average process opsumering: 

## Mean, varians og Covariance


$$Y_t = \mu + \varepsilon_t + \alpha \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$

- Udregn mean af $Y_t$

$$E[Y_t] = E[\mu]  E[\varepsilon_t] + E[\alpha \varepsilon_{t-1}]$$

- Vi ved $E[\varepsilon_t] = E[\varepsilon_{t-1}] = 0$

$$E[Y_t] = \mu$$


## Mean, varians og Covariance

$$Y_t = \mu + \varepsilon_t + \alpha \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$


- Udregn variancen
$$Var[Y_t] = E[(Y_t - \mu)^2]$$
$$=E[(\varepsilon_t + \alpha \varepsilon_{t-1})^2]$$
$$=E[\varepsilon_t^2  + 2\alpha \varepsilon_t \varepsilon_{t-1} + \alpha^2\varepsilon_{t-1}^2]$$
$$=E[\varepsilon_t^2]  + 2\alpha E[\varepsilon_t \varepsilon_{t-1}] +  \alpha^2 E[\varepsilon_{t-1}^2]$$

- Hvorfor ved vi $2\alpha E[\varepsilon_t \varepsilon_{t-1}] = 0$?
- Hvad sker der med $E[\varepsilon_t^2]$ og $E[\varepsilon_{t-1}^2]$, og hvorfor?

$$=E[\varepsilon_t^2] + \alpha^2 E[\varepsilon^2_{t-1}]$$
$$(1 + \alpha^2) \sigma^2$$


## Mean, varians og Covariance

$$Y_t = \mu + \varepsilon_t + \alpha \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$

- Udregn autocovariance mellem $Y_t$ og $Y_{t-1}$

$$Cov[Y_t,Y_{t-1}] = E[(Y_t-\mu)*(Y_{t-1} - \mu)]$$
$$E[(\varepsilon_t + \alpha \varepsilon_{t-1}) (\varepsilon_{t-1} + \alpha \varepsilon_{t-2})]$$

- Forsøg selv at tage de sidste steps!

$$\alpha E[\varepsilon_{t-1}^2]$$

$$\alpha \sigma^2$$

## Mean, Variance og Covariance

- Udregn autocovariance mellem $Y_t$ og $Y_{t-2}$

$$Cov[Y_t,Y_{t-2}] = E[(Y_t-\mu)*(Y_{t-2} - \mu)]$$
$$= E[(\varepsilon_t + \alpha \varepsilon_{t-1}) (\varepsilon_{t-2} + \alpha \varepsilon_{t-3})]$$

- Samme metode som vist på tavlen før:

$$=0$$


## MA(1) Simulation 

$$Y_t = 2 + \varepsilon_t + 0.9 \varepsilon_{t-1}$$
$$\varepsilon_t \sim IID(0, 1.5^2)$$

```{r, echo=F, fig.width=7, fig.height=4}
set.seed(1393)

n=1000 # set time period to 500 obs
e <- rnorm(n, mean=0, sd=1.5) # purely random process with mean 0 and sd = 1.5
mu=2 # we set mu= 2
y <- c()
for (t in 2:n) {
y[t] <- mu + e[t] + 0.9*e[t-1] # Note: we set alpha = 0.9$
y <- y[!is.na(y)]
}
ts.plot(y, main=expression(paste("MA(1) process with ", alpha, "=0.9")), xlab="")
```

## MA(1) Simulation 

```{r, eval=FALSE}
mean(y)# 1.919655
var(y) # 3.991872
cov(y[-length(y)],y[-1])# 1.922769
```
$$E(y_t)= \mu= 2$$
$$V(y_t)= (1+\alpha^2)\sigma^2$$
$$V(y_t)= (1+ 0.9^2)1.5^2$$
$$V(y_t)= (1+ 0.9^2)1.5^2 = 4.0725$$
$$Cov= \alpha* \sigma^2$$
$$Cov= 0.9* 1.5^2= 2.025$$



# Auto regressive Processes opsumering


$$Y_t = \mu  + \theta Y_{t-1}+ \varepsilon_t$$
$$\varepsilon_t \sim IID(0, \sigma^2)$$

- Værktøjer vi skal bruge til properties af AR-modeller: 

1. Geometriske serier. 

2. Difference ligninger. 


## Geometriske serier review - Eksempler på serier

### Eksempel 1:

$$\sum^n_{n=1}a k^n=k + k^2 + k^3 + k^4 +...+k^{n-1}$$

- Hvor $a=1$

### Eksempel 2:

$$\sum^n_{n=0} a k^{-1/2n} = k + \sqrt{k} + 1 + \frac{1}{\sqrt{k}} + \frac{1}{k} ... $$

- Hvor $a= k$

## Typer af geometriske serier

Note til senere: 

- $a= \mu$
- $k = \theta$


### Endelig serie

$\sum^n_{n=1} ak^n = a* \frac{1-k^n}{1-k}$,   $k \neq1$

### Uendelig serie

$\sum^{\infty}_{n=1} ak^n = \frac{a}{1-k}$,   $\lvert k \rvert  <1$


$\sum^{\infty}_{n=1} ak^n = na$,   $\lvert k \rvert  =1$



## Udregning

### Endelig serie

$S_n$ står for summen ved et givent n

$$S_n = \alpha + \alpha*k + \alpha*k^2 + \alpha*k^3 + ... + \alpha*k^{n-1}$$
$$k*S_n = \alpha*k + \alpha*k^2 + \alpha*k^3 + \alpha*k^4 + ... + \alpha*k^{n}$$
$$S_n - k*S_n = \alpha + (\alpha*k - \alpha*k) + (\alpha*k^2- \alpha*k^2) + ... + (\alpha*k^{n-1} - \alpha*k^{n-1}) - \alpha*k^{n}$$
$$S_n - k*S_n = \alpha - \alpha*k^{n}$$
$$S_n(1-k) = \alpha(1-k^n)$$

$$S_n = \alpha * \frac{1-k^n}{1-k}$$

## Udregning

### Uendelig serie

Hvis $\vert k \vert <  1$ når $n \to \infty$ vil udtrykket gå mod:

$$S_n = \alpha * \frac{1}{1-k}$$

Dermed kan vi skrive:

$$ \sum_{n=1}^\infty \alpha k^n = \frac{ \alpha}{1-k}$$

Hvis $\vert k \vert =  1$

Kan vi se fra tidligere slide at summen vokser med $a$ hver periode: 

$$\sum_{n=1}^\infty \alpha k^n =\alpha * n$$

## Difference equations: Math to econometrics

Lad os kigge på en 1. ordens differensligning

På 2. semester Mat havde vi følgende: 

$$x_t = ax_{t-1} + b$$
I tids serie økonometri har vi set det som en AR(1) process: 

$$y_t= \mu + \theta y_{t-1} + \varepsilon_t$$

Hvad er forskellen?

Først lad os se hvad de har tilfældes:

- Begge har en konstant $\mu$ og $b$

- Begge har en coefficient $\theta$ og $a$

- Begge har en variable der ændres over tid (discrete) $y_t$ and $x_t$


## Difference equations: Math to econometrics

**Forskellen er $\varepsilon_t$ Med definationen:**

$$\varepsilon_t \sim IID(0,\sigma^2)$$
Identical, Independent Distributed med $mean = 0$ og $Var= \sigma^2$

Senere ser vi hvilke forskelle dette giver!


## Differens equations (Math)

Lad os løse differensligningen vi havde før:

$$x_t = ax_{t-1} + b_t$$
Vi kan starte fra et givent punkt $x_0$ 

$$x_1 = ax_0 + b_1$$
$$x_2 = ax_1 + b_2 = a(ax_0 + b_1) + b_2 = a^2 x_0 + ab_1 + b_2$$
$$x_3 = ax_2 + b_3 = a(a^2x_0 + ab_1 + b_2) + b_3 = a^3x_0 + a^2 b_1 + ab_2 + b_3$$

## Differens equations (Math)

Vi kan allerede se et mønster:

$$x_t= a^t x_0 + \sum^t_{k=1} a^{t-k} b_k$$

Vi antager nu $b_k = b$ Så nu har vi en konstant ligsom i AR(1) modellen (fixed over tid).

Vi kan nu skrive det sidste led som:

$$\sum^t_{k=1}a^{t-k}b$$ 
Hvilket er en endelig geometrtisk serie! Som vi lige har kigget på!
$$\sum^t_{k=1}a^{t-k}b= b(a^{t-1} + a^{t-2} + a^{t-3} +...+a +1) =\frac{(b-ba^t)}{(1-a)}$$

## Differens equations (Math)

Derfor kan vi nu skrive: 

$$x_t= a^t (x_0 - \frac{b}{1-a}) + \frac{b}{1-a}$$

Vi kan se hvis $x_0 = \frac{b}{1-a}$ får vi løsningen $x_t= \frac{b}{1-a}$ hvilket er illustreret under:

```{r, echo=FALSE}
knitr::include_graphics(rep("diff..png", 1), dpi = 300)
```


## Differens equations (Math)

Ud fra differens ligningen $x_t = ax_{t-1} + b$ kan vi også se at hvis $x_s$ på noget tidspunkt rammer $\frac{b}{1-a}$ vil vi aldrig komme væk fra dette punkt:

$$x_{s+1} = a\frac{b}{1-a} +b = \frac{b}{1-a}$$

```{r, echo=FALSE}
knitr::include_graphics(rep("diff2.png", 1), dpi = 300)
```


## Difference equations (stability)

Vi kan kigge på stabiliteten, nøjagtigt ligsom AR(1) modellen i økonometri: 

### Case 1

$$\vert a \vert < 1$$
Vi kan se at $a^t$ går mod 0 når $t \to \infty$: 

$$x_t= a^t (x_0 - \frac{b}{1-a}) + \frac{b}{1-a}$$
Og vi ender med

$$x_t= \frac{b}{1-a}$$

## Difference equations (stability)

### Case 2

$$\vert a \vert > 1$$

- Vi kan nu se at $a^t$ Går imod $\infty$ når $t \to \infty$ og eksplodere.


### Case 3

$$\vert a \vert = 1$$

- Kommer vi til senere

- Lad os kigge på de forskellige scenarier

## Difference equations (stability)

```{r, echo=FALSE}
knitr::include_graphics(rep("diff3.png", 1), dpi = 150)
```

## AR(1) model Econometrics

Lad os kigge på AR(1) processen igen:

$$y_t= \mu + \theta y_{t-1} + \varepsilon_t$$
Hvor den eneste forskel var fejlledet: $\varepsilon_t$ lad os se på nogle eksempler om vi kan finde en forskel


## AR(1) model Exonometrics

Som eksempel bruger vi:
$$y_t= 5 +  0.5y_{t-1}+\varepsilon_t$$
Med start værdien $y_0=0$

Lad os bruge løsningen for en difference ligning for når $\lvert a \rvert < 1$

```{r, echo=FALSE, fig.height= 3.3}
suppressPackageStartupMessages(library(forecast))
# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n)

# loop to create x
for (t in 2:n) x[t] <- 5+ 0.5 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 0.5*X_t-1 + u_t"); abline( h=10, col= "red")
```
Vi kan se at stød fra $\varepsilon_t$ gør så vi aldrig bliver i $\frac{b}{1-a}$, så istedet regner vi mean!

## AR(1) model Exonometrics

$$y_t= 5 +  1.5y_{t-1}+\varepsilon_t$$


```{r, echo=FALSE, fig.height= 5}

# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n)

# loop to create x
for (t in 2:n) x[t] <- 5+ 1.5 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 1.5*X_t-1 + u_t")

```


## AR(1) model Exonometrics

$$y_t= 5 +  1y_{t-1}+\varepsilon_t$$

```{r, echo=FALSE, fig.height= 5}
# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n, mean = 0, sd= 1)

# loop to create x
for (t in 2:n) x[t] <- 5+ 1 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 1*X_t-1 + u_t")
```

## AR(1) model Exonometrics

Plottet vi så før ligner bare en lineær funktion! Hvilket det er!

Husk løsning til vores difference eq.

$$x_t = a^t x_0  + \sum^t_{k=1} a^{t-k} b_k$$
Her brugte vi løsningen til den geometriske serie til at substituere ind for $\sum^t_{k=1} a^{t-k} b_k$ Men vi antog $\lvert a \rvert < 1$

Gå tilbage til geometriske serier!

## AR(1) model Exonometrics

Vi kan derfor nu indsætte $\sum^t_{k=1} a^{t-k} b_k = ta$

$$x_t = a^t x_0  + ta$$
Vi ved $a=1$ dermed starter vi i $x_0$ og vokser linært som $t \rightarrow \infty$

## AR(1) model Exonometrics

Tilbage til AR(1) processen! Hvad er det vi kalder det når $\lvert a \rvert = 1$ aka $\lvert \theta \lvert = 1$ 

En Random walk med drift!

Lad os lave det lidt mere tydeligt ved at øge standard deviation i fejlledet!


```{r, echo=FALSE, fig.height= 5}
# Set up variables
set.seed(1234)
n <- 1000
x <- matrix(0,1000,1)
w <- rnorm(n, mean = 0, sd= 50)

# loop to create x
for (t in 2:n) x[t] <- 5+ 1 * x[t-1] + w[t]
plot(x,type='l', main = "X_t= 5 + 1*X_t-1 + u_t")
```


## AR(1) model Exonometrics (mean)

Som vi så før grundet fejlledet er der ik en løsning $\frac{b}{1-a}$ men istedet kan vi udregne mean

$$E[y_t] = E[\mu + \theta y_{t-1} + \varepsilon_t]$$
$$= \mu + \theta E[y_{t-1}] + E[\varepsilon]$$
$$= \mu + \theta E[\mu + \theta y_{t-2} + \varepsilon_{t-1}]$$
$$= \mu + \mu\theta+  \theta^2 E[y_{t-2}]$$
$$= \mu + \mu\theta+  \theta^2 E[\mu + \theta y_{t-3} + \varepsilon_{t-2}]$$
$$= \mu(1 + \theta + \theta^2 + \theta^3 + ... +\theta^\infty)$$


Så tilbage til geometriske serier, hvis $\vert \theta \vert< 1$ får vi $\frac{\mu}{1-\theta}$


## AR(1) model Exonometrics (mean)

Lad os udregne mean fra eksemplet før: 

$$y_t= 5 +  0.5y_{t-1}+\varepsilon_t$$
$$E[y_t]= \frac{5}{1-0.5} = 10$$

Lad os se på plottet igen!


## AR(1) model Exonometrics (Variance)

Da $\mu$ er en konstant vil denne ikke påvirke variancen og vi kan fjerne denne fra start. 

```{r, echo=FALSE, fig.height= 3.5}
set.seed(213)
n=500
e <- rnorm(n, mean = 0, sd = 1.5)
Y <- 2/(1-0.9)
for (t in 2:n) {
Y[t] <- 2+ 0.9*Y[t-1]+e[t]
}
ts.plot(Y, main=expression(paste("AR(1) process with ", theta, "=0.9 and" , mu ,"=2")), xlab="")
```
```{r}
var(Y)
```

## AR(1) model Exonometrics (Variance)

```{r,  echo=FALSE, fig.height= 3.5}
set.seed(213)
n=500
e <- rnorm(n, mean = 0, sd = 1.5)
Y <- 6/(1-0.9)
for (t in 2:n) {
Y[t] <- 6+ 0.9*Y[t-1]+e[t]
}
ts.plot(Y, main=expression(paste("AR(1) process with ", theta, "=0.9 and" , mu ,"=6")), xlab="")
```

```{r}
var(Y)
```


## AR(1) model Exonometrics (Variance)

$$y_t= \mu +  \theta y_{t-1}+\varepsilon_t$$
$$\varepsilon_t \sim IID(0,\sigma^2)$$
Vi antager derfor $\mu = 0$

$$V(y_t) = E[(y_t - E[y_t])^2]$$

- forklar dette step
$$V(y_t) = E[(\theta y_{t-1} + \varepsilon_t)^2]$$
$$V(y_t) = E[\varepsilon_t^2] + \theta^2 E[y_{t-1}^2]$$
$$V(y_t) = \sigma^2 + \theta^2 E[y_{t-1}^2]$$
$$V(y_t) = \sigma^2 + \theta^2 E[(\theta y_{t-2} + \varepsilon_{t-1})^2]$$

- Vi kan nu indsætte $y_{t-2}$ og gøre nøjagtigt de samme steps:
$$\sigma^2 ( 1+ \theta^2 + \theta^4 + \theta^6 + ... + \theta^{\infty})$$

## AR(1) model Exonometrics (Variance)



- Brug igen geometrisk serier
$$\frac{\sigma^2} {1 - \theta^2}$$
- IF $\lvert \theta \rvert < 1$

## AR(1) model Exonometrics (Auto covariance)


$$cov(Y_t , Y_{t-1}) = cov(\mu +  \theta Y_{t-1}+\varepsilon_t, Y_{t-1})$$

- fra statistik ved vi at $Cov(X + Y, Z) = Cov(X,Z) + Cov(Y,Z)$

$$cov(\mu,Y_{t-1}) +\theta cov(Y_{t-1},Y_{t-1}) + cov(\varepsilon_t,Y_{t-1})$$

- vi ved at $cov(\mu,Y_{t-1}) = cov(\varepsilon_t,Y_{t-1}) = 0$

- Og hvad er det nu $cov(Y_{t-1},Y_{t-1})$ er? 

$$cov(Y_t , Y_{t-1}) = \theta \frac{\sigma^2}{1-\theta^2}$$ 
- Og antagelsen fra variance skal nu bruges: $\lvert \theta \rvert < 1$




## AR(1) model Exonometrics (Auto covariance)

$$cov(Y_t , Y_{t-2}) = cov(\mu +  \theta Y_{t-1}+\varepsilon_t, Y_{t-2})$$

$$cov(\mu,Y_{t-2}) +\theta cov(Y_{t-1},Y_{t-2}) + cov(\varepsilon_t,Y_{t-1})$$

- vi ved at $cov(\mu,Y_{t-1}) = cov(\varepsilon_t,Y_{t-1}) = 0$

- Og vi kender $cov(Y_{t-1},Y_{t-2})$ som vi fandt på sidste slide. 

$$cov(Y_t , Y_{t-2}) = \theta^2 \frac{\sigma^2}{1-\theta^2}$$ 

- Da vi bruger covariancen med antagelsen, gælder den stadig: $\lvert \theta \rvert < 1$

- Derfor ACF aftager over tid når i plotter en AR-model.


# Done

## Cheatsheet

+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Name        | AR(1)                             | MA(1)                    | AR(1) (RW1)             | AR(1) (RW2)             | AR(1) (RW3)             |
|             |                                   |                          |                         |                         |                         |
|             | $$                                |                          | $$                      | $$                      | $$                      |
|             | \lvert\theta \lvert < 1           |                          | \lvert\theta \lvert = 1 | \lvert\theta \lvert = 1 | \lvert\theta \lvert = 1 |
|             | $$                                |                          | $$                      | $$                      | $$                      |
+=============+===================================+==========================+=========================+=========================+=========================+
| Mean        | $$                                | $$                       | $$                      | $$                      | $$                      |
|             | \frac{\mu}{1-\theta}              | \mu                      | Y_0                     | Y_0 + T\mu              | Y_0 + T\mu + t          |
|             | $$                                | $$                       | $$                      | $$                      | $$                      |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Var         | $$                                | $$                       | $$                      | $$                      | $$                      |
|             | \frac{\sigma^2}{1-\theta^2}       | (1 + \alpha ^2) \sigma^2 | T \sigma^2              | T \sigma^2              | T \sigma^2              |
|             | $$                                | $$                       | $$                      | $$                      | $$                      |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Cov         | $$                                | $$                       | $$                      | $$                      | $$                      |
|             | \theta\frac{\sigma^2}{1-\theta^2} | \alpha \sigma^2          | T \sigma^2              | T \sigma^2              | T \sigma^2              |
|             | $$                                | $$                       | $$                      | $$                      | $$                      |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+
| Stationary? |           YES!                    | YES!                     | NO!                     | NO!                     | NO!                     |
+-------------+-----------------------------------+--------------------------+-------------------------+-------------------------+-------------------------+

- RW1 = Random walk uden drift og trend

- RW2 = Random walk med drift

- RW3 = Random walk med drift og trend



